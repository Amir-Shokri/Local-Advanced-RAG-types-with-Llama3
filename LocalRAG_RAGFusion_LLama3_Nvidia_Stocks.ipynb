{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCW_531Xbbgi"
      },
      "source": [
        "# **RAG-Fusion Retriever**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COYsRAXEbf8M"
      },
      "source": [
        "## Install Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDdxDxlPP4d4",
        "outputId": "fe0403a9-2ee6-4122-9014-d362d3af5f2c"
      },
      "outputs": [],
      "source": [
        "! pip install sentence-transformers\n",
        "! pip install --q unstructured langchain\n",
        "! pip install --q \"unstructured[all-docs]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xLtZ56oabKr_",
        "outputId": "ea13d52f-54c9-4c2d-cf9b-937d055dbaae"
      },
      "outputs": [],
      "source": [
        "! pip install langchain_community fastembed chromadb ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIol7pjebi9S"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Gc4R_kTQbkOa"
      },
      "outputs": [],
      "source": [
        "# Define the directory where your PDFs are stored\n",
        "pdf_directory = \"C:/Users/ili/Downloads/test_rag\"\n",
        "save_dir = pdf_directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvbDQxfMrqRA"
      },
      "source": [
        "## **1. Extract Texts from PDFs**\n",
        "use **PyPDFLoader** from LangChain_Community to extract textual data <br>\n",
        "from **Multipple PDFs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItRz_bpfbw6R",
        "outputId": "8e89396f-b614-4e3a-bdaa-7f32a14beb6f"
      },
      "outputs": [],
      "source": [
        "# general\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# Lancgain\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Get a list of all PDF files in the directory\n",
        "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
        "\n",
        "# Initialize lists to hold pages from Nvidia and Tesla PDFs separately\n",
        "nvidia_pages = []\n",
        "\n",
        "\n",
        "# Iterate through each PDF file and load it\n",
        "for pdf_file in pdf_files:\n",
        "    file_path = os.path.join(pdf_directory, pdf_file)\n",
        "    print(f\"Processing file: {file_path}\\n\")\n",
        "\n",
        "    # Load the PDF and split it into pages\n",
        "    loader = PyPDFLoader(file_path=file_path)\n",
        "    pages = loader.load()\n",
        "\n",
        "\n",
        "    nvidia_pages.extend(pages)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy1CaNO_rye0",
        "outputId": "24662704-f651-47a3-9a8e-e82a6e05e41e"
      },
      "outputs": [],
      "source": [
        "# print out the first page of the first document for each category as an example\n",
        "if nvidia_pages:\n",
        "    print(\"=========================================\")\n",
        "    print(\"First page of the first Nvidia document:\")\n",
        "    print(\"=========================================\\n\")\n",
        "    print(nvidia_pages[0].page_content)\n",
        "else:\n",
        "    print(\"No Nvidia pages found in the PDFs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCG-f1UjsAJB"
      },
      "source": [
        "## **2. Split Text**\n",
        "We'll use RecursiveCharacterTextSplitter to break down the large text bodies from the PDFs into manageable chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YGB7zjzsCs-"
      },
      "source": [
        "### 2.1 Text Splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UhzkR7Q-sNRr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
        "\n",
        "# Split text into chunks for Nvidia pages\n",
        "nvidia_text_chunks = []\n",
        "for page in nvidia_pages:\n",
        "    chunks = text_splitter.split_text(page.page_content)\n",
        "    nvidia_text_chunks.extend(chunks)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IygoYM59sfjU"
      },
      "source": [
        "### 2.2 Add Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AGvtsHvlsfNv"
      },
      "outputs": [],
      "source": [
        "# Example metadata management (customize as needed)\n",
        "def add_metadata(chunks, doc_title):\n",
        "    metadata_chunks = []\n",
        "    for chunk in chunks:\n",
        "        metadata = {\n",
        "            \"title\": doc_title,\n",
        "            \"author\": \"company\",  # Update based on document data\n",
        "            \"date\": str(datetime.date.today())\n",
        "        }\n",
        "        metadata_chunks.append({\"text\": chunk, \"metadata\": metadata})\n",
        "    return metadata_chunks\n",
        "\n",
        "# Add metadata to Nvidia chunks\n",
        "nvidia_chunks_with_metadata = add_metadata(nvidia_text_chunks, \"NVIDIA Financial Report\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKmZYvXXs38W"
      },
      "source": [
        "## **3. Create Embedding from text chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-VuDYvTs3ho",
        "outputId": "aa30f778-107f-4797-bca9-a9e8b70938a4"
      },
      "outputs": [],
      "source": [
        "! ollama pull nomic-embed-text:v1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e8AtjvKx8M-",
        "outputId": "f5b9513c-bbe9-437f-dc9a-48d7f666d76f"
      },
      "outputs": [],
      "source": [
        "! ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V7Y3zi98yQ9p"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "# Function to generate embeddings for text chunks\n",
        "def generate_embeddings(text_chunks, model_name='nomic-embed-text:v1.5'):\n",
        "    embeddings = []\n",
        "    for chunk in text_chunks:\n",
        "        # Generate the embedding for each chunk\n",
        "        embedding = ollama.embeddings(model=model_name, prompt=chunk)\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suGAz6gZyY5N",
        "outputId": "d3d492e2-5e2b-4c86-dd19-1a2158aafebd"
      },
      "outputs": [],
      "source": [
        "# Example: Embed Nvidia text chunks\n",
        "nvidia_texts = [chunk[\"text\"] for chunk in nvidia_chunks_with_metadata]\n",
        "nvidia_embeddings = generate_embeddings(nvidia_texts)\n",
        "\n",
        "nvidia_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY5fUbnKzXOK"
      },
      "source": [
        "## **4. Store and Use Embeddings in Chroma DB**\n",
        "After generating the embeddings, you can store them in Chroma DB for efficient retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOmvf_rQzbyl"
      },
      "source": [
        "### **CHROMADB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYrB3L-0zWoU"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# Wrap Nvidia texts with their respective metadata into Document objects\n",
        "nvidia_documents = [Document(page_content=chunk['text'], metadata=chunk['metadata']) for chunk in nvidia_chunks_with_metadata]\n",
        "\n",
        "\n",
        "# Add Nvidia embeddings to the database\n",
        "nvidia_vector_db = Chroma.from_documents(documents=nvidia_documents,\n",
        "                      embedding=OllamaEmbeddings(model=\"nomic-embed-text:v1.5\",show_progress=False),\n",
        "                      collection_name=\"nvidia-local-rag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2J2xzH04BDj"
      },
      "source": [
        "## **5. Query Processing RAG-Fusion Retriever:**\n",
        "\n",
        "Implement a RAG-Fusion retriever using Chroma DB. Fetch the most relevant chunks from the database based on user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYJUMWRH4Xxr"
      },
      "outputs": [],
      "source": [
        "# LLM from Ollama\n",
        "local_model = \"llama3:latest\"\n",
        "llm = ChatOllama(model=local_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N81nz4Vx4pjP"
      },
      "outputs": [],
      "source": [
        " # RAG-Fusion: Related\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
        "    Generate multiple search queries related to: {question} \\n\n",
        "    Output (4 queries):\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4fXYraTl5AQ-"
      },
      "outputs": [],
      "source": [
        "retriever = MultiQueryRetriever.from_llm(\n",
        "                                          nvidia_vector_db.as_retriever(),\n",
        "                                          ChatOllama(model=local_model),\n",
        "                                          prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## re-rank search results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
        "        and an optional parameter k used in the RRF formula \"\"\"\n",
        "    \n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = retriever | reciprocal_rank_fusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = '''What are the main revenue drivers for Nvidia this fiscal year?'''\n",
        "\n",
        "docs = retrieval_chain_rag_fusion.invoke({\"question\": questions},)\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mAfDnFn45et8"
      },
      "outputs": [],
      "source": [
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_chain_rag_fusion, \n",
        "     \"question\": RunnablePassthrough()} \n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "cw4Ta2yHaJOk",
        "outputId": "eb36eda9-66a4-450d-b574-3026a8691874"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "questions = '''What are the main revenue drivers for Nvidia this fiscal year?'''\n",
        "display(Markdown(final_rag_chain.invoke(questions)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "nPxoOnYIak9L",
        "outputId": "906af013-0119-40d2-b341-9b0409327a1a"
      },
      "outputs": [],
      "source": [
        "questions = '''Can you some financial advise on Nvidia Stock to the future? should people consider buying it?'''\n",
        "display(Markdown(final_rag_chain.invoke(questions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzrP5UR9NYj-"
      },
      "source": [
        "#  Chatting with Local RAG - Hugging Face Embedding + Llama 3 -> Improve Speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StE4ZVClf465"
      },
      "outputs": [],
      "source": [
        "from langchain_community.llms.ollama import Ollama\n",
        "\n",
        "\n",
        "local_model = \"llama3:latest\"\n",
        "cached_llm = Ollama(model=local_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **CHROMADB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGKRyTC7QFcD"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a smaller Hugging Face embedding model\n",
        "#hf_embedding_model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Wrap the Hugging Face model for use with LangChain\n",
        "#hf_embeddings = HuggingFaceEmbeddings(model=SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2'), show_progress=True)\n",
        "\n",
        "# Wrap Nvidia texts with their respective metadata into Document objects\n",
        "nvidia_documents = [Document(page_content=chunk['text'], metadata=chunk['metadata']) for chunk in nvidia_chunks_with_metadata]\n",
        "\n",
        "model_name = \"sentence-transformers/paraphrase-MiniLM-L6-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "# Add Nvidia embeddings to the database using the smaller Hugging Face model\n",
        "nvidia_vector_db_hf = Chroma.from_documents(documents=nvidia_documents,\n",
        "                      embedding=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\"),\n",
        "                      collection_name=\"nvidia-local-rag-384\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever_hf = MultiQueryRetriever.from_llm(\n",
        "                                          nvidia_vector_db_hf.as_retriever(),\n",
        "                                          ChatOllama(model=local_model),\n",
        "                                          prompt=QUERY_PROMPT\n",
        ")\n",
        "\n",
        "# RAG prompt\n",
        "template = \"\"\"Answer the question based ONLY on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt_hf = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "retrieval_chain_rag_fusion_hf = retriever_hf | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions = '''What are the main revenue drivers for Nvidia this fiscal year?'''\n",
        "\n",
        "docs_hf = retrieval_chain_rag_fusion_hf.invoke({\"question\": questions},)\n",
        "len(docs_hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_rag_chain_hf = (\n",
        "    {\"context\": retrieval_chain_rag_fusion_hf, \n",
        "     \"question\": RunnablePassthrough()} \n",
        "    | prompt_hf\n",
        "    | cached_llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "questions = '''What are the main revenue drivers for Nvidia this fiscal year?'''\n",
        "display(Markdown(final_rag_chain_hf.invoke(questions)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12a25cdd81814ce1bee80c738c414c98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13da2f0f8f754cf096bafcdab7cdf49d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88b8e56f52a048d1ae035442503cc4eb",
              "IPY_MODEL_1451268024e14935bc365dbb1dae58f5",
              "IPY_MODEL_7c14cf46ff124fc894b60f1f800f0321"
            ],
            "layout": "IPY_MODEL_28c10439917c415e89ebec4b40b0c690"
          }
        },
        "1451268024e14935bc365dbb1dae58f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc5ccfa3770a4e6f9930a5a5effc86c5",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d271b3002f5439d8be17a55e445ae23",
            "value": 5
          }
        },
        "28c10439917c415e89ebec4b40b0c690": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301dd2e6b85e4f268848c7d829f40efc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6881e52173db47c4b7129ba6b37d39ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69289cc06bba40c1ba3730f122f88e01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c14cf46ff124fc894b60f1f800f0321": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69289cc06bba40c1ba3730f122f88e01",
            "placeholder": "​",
            "style": "IPY_MODEL_6881e52173db47c4b7129ba6b37d39ab",
            "value": " 5/5 [00:00&lt;00:00, 186.05it/s]"
          }
        },
        "88b8e56f52a048d1ae035442503cc4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12a25cdd81814ce1bee80c738c414c98",
            "placeholder": "​",
            "style": "IPY_MODEL_301dd2e6b85e4f268848c7d829f40efc",
            "value": "Fetching 5 files: 100%"
          }
        },
        "8d271b3002f5439d8be17a55e445ae23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc5ccfa3770a4e6f9930a5a5effc86c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
